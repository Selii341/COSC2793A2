{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Selii341/COSC2793A2/blob/main/A2_Sisi_Zhang_S4000130.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f4571ae",
      "metadata": {
        "id": "3f4571ae"
      },
      "source": [
        "# 1.Data Prepeartaion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf776491",
      "metadata": {
        "id": "bf776491"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a51d33dd",
      "metadata": {
        "id": "a51d33dd"
      },
      "source": [
        "## 1.1 Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1094b3f",
      "metadata": {
        "id": "a1094b3f",
        "outputId": "e1b55787-2886-43cd-8fbb-530d3a81c343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mA1\u001b[m\u001b[m                           \u001b[34mUCI-electricity\u001b[m\u001b[m\r\n",
            "A2_Sisi Zhang_S4000130.ipynb \u001b[34m__MACOSX\u001b[m\u001b[m\r\n"
          ]
        }
      ],
      "source": [
        "#Unzip the File\n",
        "!unzip -q -o UCI-electricity.zip\n",
        "!rm UCI-electricity.zip\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac96c33e",
      "metadata": {
        "id": "ac96c33e"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"UCI-electricity/UCI_data.csv\",delimiter=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "676f6f93",
      "metadata": {
        "id": "676f6f93",
        "outputId": "694adaab-a0ec-4cd8-f3d6-feec4de60976"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>T1</th>\n",
              "      <th>RH_1</th>\n",
              "      <th>T2</th>\n",
              "      <th>RH_2</th>\n",
              "      <th>T3</th>\n",
              "      <th>RH_3</th>\n",
              "      <th>T4</th>\n",
              "      <th>RH_4</th>\n",
              "      <th>T5</th>\n",
              "      <th>...</th>\n",
              "      <th>RH_9</th>\n",
              "      <th>T_out</th>\n",
              "      <th>Press_mm_hg</th>\n",
              "      <th>RH_out</th>\n",
              "      <th>Windspeed</th>\n",
              "      <th>Visibility</th>\n",
              "      <th>Tdewpoint</th>\n",
              "      <th>rv1</th>\n",
              "      <th>rv2</th>\n",
              "      <th>TARGET_energy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-04-19 20:30:00</td>\n",
              "      <td>22.200000</td>\n",
              "      <td>39.500000</td>\n",
              "      <td>20.566667</td>\n",
              "      <td>37.656667</td>\n",
              "      <td>22.230000</td>\n",
              "      <td>37.030000</td>\n",
              "      <td>22.318571</td>\n",
              "      <td>36.610000</td>\n",
              "      <td>20.633333</td>\n",
              "      <td>...</td>\n",
              "      <td>33.90</td>\n",
              "      <td>9.70</td>\n",
              "      <td>766.100000</td>\n",
              "      <td>65.5</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>3.350000</td>\n",
              "      <td>24.061869</td>\n",
              "      <td>24.061869</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-03-05 04:40:00</td>\n",
              "      <td>20.356667</td>\n",
              "      <td>37.126667</td>\n",
              "      <td>17.566667</td>\n",
              "      <td>40.230000</td>\n",
              "      <td>20.890000</td>\n",
              "      <td>37.663333</td>\n",
              "      <td>18.700000</td>\n",
              "      <td>36.260000</td>\n",
              "      <td>18.463333</td>\n",
              "      <td>...</td>\n",
              "      <td>41.09</td>\n",
              "      <td>0.30</td>\n",
              "      <td>740.333333</td>\n",
              "      <td>99.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>41.333333</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>4.622052</td>\n",
              "      <td>4.622052</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-03-14 12:40:00</td>\n",
              "      <td>20.926667</td>\n",
              "      <td>38.790000</td>\n",
              "      <td>21.100000</td>\n",
              "      <td>35.526667</td>\n",
              "      <td>21.600000</td>\n",
              "      <td>36.290000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>34.826667</td>\n",
              "      <td>18.100000</td>\n",
              "      <td>...</td>\n",
              "      <td>38.76</td>\n",
              "      <td>4.40</td>\n",
              "      <td>768.466667</td>\n",
              "      <td>72.0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>22.666667</td>\n",
              "      <td>-0.266667</td>\n",
              "      <td>5.635898</td>\n",
              "      <td>5.635898</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-01-22 15:30:00</td>\n",
              "      <td>18.290000</td>\n",
              "      <td>38.900000</td>\n",
              "      <td>17.290000</td>\n",
              "      <td>39.260000</td>\n",
              "      <td>18.390000</td>\n",
              "      <td>39.326667</td>\n",
              "      <td>16.100000</td>\n",
              "      <td>38.790000</td>\n",
              "      <td>16.100000</td>\n",
              "      <td>...</td>\n",
              "      <td>39.20</td>\n",
              "      <td>3.35</td>\n",
              "      <td>760.600000</td>\n",
              "      <td>82.0</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>49.216445</td>\n",
              "      <td>49.216445</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-02-10 00:40:00</td>\n",
              "      <td>22.290000</td>\n",
              "      <td>42.333333</td>\n",
              "      <td>21.600000</td>\n",
              "      <td>40.433333</td>\n",
              "      <td>22.666667</td>\n",
              "      <td>43.363333</td>\n",
              "      <td>19.100000</td>\n",
              "      <td>40.900000</td>\n",
              "      <td>19.290000</td>\n",
              "      <td>...</td>\n",
              "      <td>43.73</td>\n",
              "      <td>3.20</td>\n",
              "      <td>738.900000</td>\n",
              "      <td>88.0</td>\n",
              "      <td>7.333333</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>47.617579</td>\n",
              "      <td>47.617579</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  date         T1       RH_1         T2       RH_2         T3  \\\n",
              "0  2016-04-19 20:30:00  22.200000  39.500000  20.566667  37.656667  22.230000   \n",
              "1  2016-03-05 04:40:00  20.356667  37.126667  17.566667  40.230000  20.890000   \n",
              "2  2016-03-14 12:40:00  20.926667  38.790000  21.100000  35.526667  21.600000   \n",
              "3  2016-01-22 15:30:00  18.290000  38.900000  17.290000  39.260000  18.390000   \n",
              "4  2016-02-10 00:40:00  22.290000  42.333333  21.600000  40.433333  22.666667   \n",
              "\n",
              "        RH_3         T4       RH_4         T5  ...   RH_9  T_out  Press_mm_hg  \\\n",
              "0  37.030000  22.318571  36.610000  20.633333  ...  33.90   9.70   766.100000   \n",
              "1  37.663333  18.700000  36.260000  18.463333  ...  41.09   0.30   740.333333   \n",
              "2  36.290000  21.000000  34.826667  18.100000  ...  38.76   4.40   768.466667   \n",
              "3  39.326667  16.100000  38.790000  16.100000  ...  39.20   3.35   760.600000   \n",
              "4  43.363333  19.100000  40.900000  19.290000  ...  43.73   3.20   738.900000   \n",
              "\n",
              "   RH_out  Windspeed  Visibility  Tdewpoint        rv1        rv2  \\\n",
              "0    65.5   3.500000   40.000000   3.350000  24.061869  24.061869   \n",
              "1    99.0   1.000000   41.333333   0.100000   4.622052   4.622052   \n",
              "2    72.0   6.000000   22.666667  -0.266667   5.635898   5.635898   \n",
              "3    82.0   5.500000   41.000000   0.500000  49.216445  49.216445   \n",
              "4    88.0   7.333333   56.000000   1.400000  47.617579  47.617579   \n",
              "\n",
              "   TARGET_energy  \n",
              "0             60  \n",
              "1             50  \n",
              "2             80  \n",
              "3             40  \n",
              "4             60  \n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "167d0081",
      "metadata": {
        "id": "167d0081"
      },
      "source": [
        "The ID columns is useless here,We will drop it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d52dde95",
      "metadata": {
        "id": "d52dde95"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "828eb480",
      "metadata": {
        "id": "828eb480"
      },
      "source": [
        "Describing the dataset. The dataset consists of examples from 193 different countries from different years. It is shown that minimum value for year is 2002 and maximum is 2017; it consists of 15 year period. Also the status column is binary. The population column has huge values in it, which is expected. The values can be showed as millions.However, The SLS dsitribution will be very skewed and some minumum value is zero,we will check them later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c42d59db",
      "metadata": {
        "scrolled": true,
        "id": "c42d59db"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e7498e2",
      "metadata": {
        "id": "7e7498e2"
      },
      "source": [
        "No missing values and all datatype is constructed well.All float are the countinous number,some of int is the Category number i.e. Country,Year and Status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7693d52d",
      "metadata": {
        "id": "7693d52d"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c46570e0",
      "metadata": {
        "id": "c46570e0"
      },
      "source": [
        "Now checking shape of dataset which will return the value of number of Rows and Columns.There are 24 Columns and 2071 rows."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b84ff7dc",
      "metadata": {
        "id": "b84ff7dc"
      },
      "source": [
        "## 1.2 Cleaning  Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a3d3178",
      "metadata": {
        "id": "7a3d3178"
      },
      "outputs": [],
      "source": [
        "#Status checking\n",
        "print(set((data['Status']).astype(int)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2fe4d2f",
      "metadata": {
        "scrolled": true,
        "id": "e2fe4d2f"
      },
      "outputs": [],
      "source": [
        "#Missing value\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5c49b19",
      "metadata": {
        "id": "a5c49b19"
      },
      "outputs": [],
      "source": [
        "#Population Zoom Out\n",
        "data['Population'] = data['Population']/1000000\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4c69252",
      "metadata": {
        "id": "b4c69252"
      },
      "outputs": [],
      "source": [
        "#Rename the columns thinness  1-19 years to match the feature description\n",
        "data.rename(columns={'Thinness1-19years':'Thinness10-19years'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9efca479",
      "metadata": {
        "id": "9efca479"
      },
      "outputs": [],
      "source": [
        "#Checking the data again\n",
        "data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "915d4f96",
      "metadata": {
        "id": "915d4f96"
      },
      "source": [
        "In total, AdultMortality, AdultMortality-Male and\n",
        "AdultMortality-Female are very similar in each value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a435398",
      "metadata": {
        "id": "8a435398"
      },
      "outputs": [],
      "source": [
        "#Looking at the name of columns which can be used later.\n",
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "969107a5",
      "metadata": {
        "id": "969107a5"
      },
      "outputs": [],
      "source": [
        "#Checking any repeat rows which are meaningless.\n",
        "data.duplicated().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eadb468e",
      "metadata": {
        "id": "eadb468e"
      },
      "source": [
        "# 2. EDA Data distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77ba5012",
      "metadata": {
        "id": "77ba5012"
      },
      "outputs": [],
      "source": [
        "#We will first drop the ID column as it cannot be used in our Regression model.\n",
        "data = data.drop(['ID'], axis=1)\n",
        "data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6073122",
      "metadata": {
        "id": "a6073122"
      },
      "source": [
        "## 2.1 Overall Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8c67329",
      "metadata": {
        "id": "e8c67329"
      },
      "outputs": [],
      "source": [
        "#Histograms\n",
        "plt.figure(figsize=(30,40))\n",
        "for i, col in enumerate(data.columns):\n",
        "    plt.subplot(6,4,i+1)\n",
        "    plt.hist(data[col], alpha=0.3, color='b', density=True)\n",
        "    plt.title(col)\n",
        "    plt.xticks(rotation='vertical')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b893306d",
      "metadata": {
        "id": "b893306d"
      },
      "source": [
        "These are the different histograms for all 20+ quantitative features. Some of these features will be looked into deeper.\n",
        "Attribute country/status are categorical variables.\n",
        "Many attributes are heavily skewed. e.g. SLS, %Expend, Measles, Under5LS, HIV-AISD...\n",
        "Attributes Poilo,Diphtheria,thiness10-19/5-9... has values that are far from the majority values.\n",
        "Target variable is lightly skewed and some dstirbution may used log transformation i.e.polio,dihtheria,Income and schooling.Further investigations are needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "991cce63",
      "metadata": {
        "id": "991cce63"
      },
      "outputs": [],
      "source": [
        "#Distance plot for Life Expectancy column\n",
        "sns.distplot(data['TARGET_LifeExpectancy'])\n",
        "\n",
        "#Find the skewness of life expectancy\n",
        "print(data['TARGET_LifeExpectancy'].skew())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd0cf2f3",
      "metadata": {
        "id": "bd0cf2f3"
      },
      "source": [
        "Life expectancy is slightly left-skewed. This variable will not be transformed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d2d0c43",
      "metadata": {
        "id": "1d2d0c43"
      },
      "outputs": [],
      "source": [
        "#Colums to visualize the skews and outliers except the status which will be checked later.\n",
        "columns = {1: 'TARGET_LifeExpectancy', 2: 'Country', 3: 'Year', 4: 'AdultMortality',\n",
        "        5: 'AdultMortality-Male' , 6: 'AdultMortality-Female', 7: 'SLS',\n",
        "       8: 'Alcohol', 9: 'PercentageExpenditure', 10: 'Measles', 11: 'BMI', 12: 'Under5LS',\n",
        "       13: 'Polio', 14: 'TotalExpenditure', 15: 'Diphtheria', 16: 'HIV-AIDS',\n",
        "       17: 'GDP', 18: 'Population',19: 'Thinness10-19years', 20: 'Thinness5-9years',\n",
        "           21: 'IncomeCompositionOfResources', 22: 'Schooling'}\n",
        "\n",
        "#Looking their skews\n",
        "for i, column in columns.items():\n",
        "                     print(i,columns[i],data[column].skew())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55683484",
      "metadata": {
        "id": "55683484"
      },
      "source": [
        "|skew|>0.5will be considered to transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e154a0f",
      "metadata": {
        "id": "1e154a0f"
      },
      "outputs": [],
      "source": [
        "#Box Plots to visualize the outliers\n",
        "plt.figure(figsize=(28, 30))\n",
        "\n",
        "for i, column in columns.items():\n",
        "                     plt.subplot(4,6,i)\n",
        "                     sns.boxplot(data[column], orient='v')\n",
        "                     plt.title(column)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ea7cf44",
      "metadata": {
        "id": "3ea7cf44"
      },
      "source": [
        "Now it is reasonable that those highly skewed attribute have lots outliers.The ouliers need be removed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb5e9b46",
      "metadata": {
        "id": "cb5e9b46"
      },
      "source": [
        "## 2.2 Categorical Features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed56032e",
      "metadata": {
        "id": "ed56032e"
      },
      "source": [
        "We will use frequency tables to look at the number of times a value was repeated in each of the categorical feature. There are 2 different categorical features, so each one is explored below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24d9441a",
      "metadata": {
        "id": "24d9441a"
      },
      "outputs": [],
      "source": [
        "#Checking the numbers in status class.\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "data[\"Status\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4642118",
      "metadata": {
        "id": "e4642118"
      },
      "outputs": [],
      "source": [
        "# see the impact of \"status\" on \"Life Expectancy\"\n",
        "\n",
        "# set figure size\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.boxplot(x = data['Status'], y = data['TARGET_LifeExpectancy'], data = data)\n",
        "plt.xlabel(\"Status\")\n",
        "plt.ylabel(\"TARGET_LifeExpectancy\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd9998ef",
      "metadata": {
        "id": "dd9998ef"
      },
      "source": [
        "This boxplot shows that country status is 1(developed) countries are stable because the range of the live expectancy age of developed countries is short, also the status 0 (developing) countries may have very different life expenctacy rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44171334",
      "metadata": {
        "id": "44171334"
      },
      "outputs": [],
      "source": [
        "pd.set_option(\"display.max_rows\", None)\n",
        "data[\"Country\"].value_counts(ascending=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10013df3",
      "metadata": {
        "id": "10013df3"
      },
      "source": [
        "## 2.3 Ploting Heat Map to show the relation between data values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42fe64b9",
      "metadata": {
        "id": "42fe64b9"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(11, 8))\n",
        "sns.heatmap(ax=ax,annot=True, fmt = \".2f\",data=data.corr())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14bdab66",
      "metadata": {
        "id": "14bdab66"
      },
      "source": [
        "Correlation map of the features. 1 means positively correlated, -1 means negatively correlated. Income and schooling are highly positively correlated which is bigger than 0.6. Also the three adult mortalities are highly negatively correlated. I would like to keep one of the features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "898fd03d",
      "metadata": {
        "id": "898fd03d"
      },
      "source": [
        "## 2.4  Ploting Pair plot to see both distribution of single variables and relationships between two variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aa10e39",
      "metadata": {
        "id": "5aa10e39"
      },
      "outputs": [],
      "source": [
        "#Pair Plot\n",
        "g = sns.PairGrid(data, vars=['Country', 'Year', 'AdultMortality',\n",
        "       'SLS', 'Alcohol','PercentageExpenditure', 'Measles', 'BMI', 'Under5LS', 'Polio',\n",
        "       'TotalExpenditure', 'Diphtheria', 'HIV-AIDS', 'GDP', 'Population',\n",
        "       'Thinness10-19years', 'Thinness5-9years',\n",
        "       'IncomeCompositionOfResources', 'Schooling','TARGET_LifeExpectancy'], hue=\"Status\")\n",
        "g.map(sns.scatterplot)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fc71149",
      "metadata": {
        "id": "7fc71149"
      },
      "source": [
        "In this plot, the developed county is highly influenced at the GDP, Income, Schooling, Lifespan and Expenditures. But fewer impacts on mortalities, Thinness and other disease, this is reasonable.\n",
        "On the other hands, some attributes are linear (ex. target vs schooling and adult), some are performed as non-linear (ex.target vs country and year) and most of the plots are not clear to performed as linear relationship may be considered in polynomial distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de12f9bc",
      "metadata": {
        "id": "de12f9bc"
      },
      "outputs": [],
      "source": [
        "#Just for decalring the representation of orange and blue.\n",
        "count_vals = list(data.columns)[2:]\n",
        "for f in count_vals:\n",
        "    sns.lmplot(data=data, x=f, y=\"TARGET_LifeExpectancy\", hue=\"Status\", markers=[\"o\", \"x\"])\n",
        "    nombre = f.translate({ord(c): None for c in string.whitespace})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03552c2c",
      "metadata": {
        "id": "03552c2c"
      },
      "outputs": [],
      "source": [
        "data.groupby('Year').mean()['TARGET_LifeExpectancy'].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71c2b7ae",
      "metadata": {
        "id": "71c2b7ae"
      },
      "source": [
        "Through the graph, we found that life expectancy was at its lowest in 2002 and began to increase annually thereafter, with a particularly rapid increase starting in 2009. By consulting information, we learned that many natural disasters occurred worldwide in 2001-2002, and conflicts in some countries may have led to a decline in human life expectancy. After 2009, with the development of the economy and medicine, human life expectancy began to increase regularly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7782f0b3",
      "metadata": {
        "id": "7782f0b3"
      },
      "source": [
        "### Other Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff925e47",
      "metadata": {
        "scrolled": true,
        "id": "ff925e47"
      },
      "outputs": [],
      "source": [
        "#I choosing the high correralted attribute\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(2,3,1)\n",
        "plt.scatter(x = data['Schooling'], y = data['GDP'])\n",
        "plt.title(\"Schooling Vs GDP\")\n",
        "\n",
        "plt.subplot(2,3,2)\n",
        "plt.scatter(x = data['Schooling'], y = data['IncomeCompositionOfResources'])\n",
        "plt.title(\"Schooling Vs IncomeCompositionOfResources\")\n",
        "\n",
        "plt.subplot(2,3,3)\n",
        "plt.scatter(x = data['Schooling'], y = data['HIV-AIDS'])\n",
        "plt.title(\"Schooling Vs HIV-AIDS\")\n",
        "\n",
        "plt.subplot(2,3,4)\n",
        "plt.scatter(x = data['Schooling'], y = data['Thinness10-19years'])\n",
        "plt.title(\"Schooling Vs Thinness10-19years\")\n",
        "\n",
        "plt.subplot(2,3,5)\n",
        "plt.scatter(x = data['Schooling'], y = data['Thinness5-9years'])\n",
        "plt.title(\"Schooling Vs Thinness5-9years\")\n",
        "\n",
        "plt.subplot(2,3,6)\n",
        "plt.scatter(x = data['Schooling'], y = data['AdultMortality'])\n",
        "plt.title(\"Schooling Vs AdultMortality\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42b906fc",
      "metadata": {
        "id": "42b906fc"
      },
      "source": [
        "From the graph, we can observe a positive correlation between education and life expectancy. Countries with better education tend to have higher life expectancies. Additionally, there is a negative correlation between education and adult mortality rates, as well as thinness among age groups 5-19. This indicates that countries with lower levels of education tend to have higher adult mortality rates and a greater number of individuals experiencing weakness within the specified age groups."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce4401a9",
      "metadata": {
        "id": "ce4401a9"
      },
      "source": [
        "# 3. Feature Transformation\n",
        "From EDA, the following features will be transformed.\n",
        "\n",
        "4 AdultMortality 1.0556897487743073\n",
        "\n",
        "(AdultMortality-Male 1.0857582650450177,AdultMortality-Female 1.0456903472563532)\n",
        "\n",
        "7 SLS 8.918731450442381\n",
        "\n",
        "8 Alcohol 0.5931174974663874  The skewness is good but becuase of the right skewness illustrated by the histrogram, we will transform this variable.\n",
        "\n",
        "9 PercentageExpenditure 4.783377334278013\n",
        "\n",
        "10 Measles 11.393260539407844\n",
        "\n",
        "12 Under5LS 8.6091945414921\n",
        "\n",
        "13 Polio -2.0694294983318042\n",
        "\n",
        "15 Diphtheria -2.0903544010792072\n",
        "\n",
        "16 HIV-AIDS 5.917949827757557\n",
        "\n",
        "17 GDP 3.582881011151258\n",
        "\n",
        "18 Population 15.312554068769145\n",
        "\n",
        "19 Thinness10-19years 1.7989918792788289\n",
        "\n",
        "20 Thinness5-9years 1.8625650916241798\n",
        "\n",
        "21 IncomeCompositionOfResources -0.9939962573022197\n",
        "\n",
        "22 Schooling -2.265334672771308\n",
        "\n",
        "Outlier must need to be Removed, because these values are not going to contribute while building model even they are going to despotize our Model. We can transform the data using IQR method and logarithmic scale. The advantage is that we can \"normalize\" the data and eliminate outliers. We cannot apply it for variables with a value less than or equal to 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bad8bb62",
      "metadata": {
        "id": "bad8bb62"
      },
      "source": [
        "## 3.1  Outliers removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76633ccd",
      "metadata": {
        "id": "76633ccd"
      },
      "outputs": [],
      "source": [
        "# Transform features using the IQR method for outlier removal\n",
        "from tabulate import tabulate\n",
        "#IQR method for removal\n",
        "def determine_outlier_thresholds_iqr(dataframe, col_name, th1=0.25, th3=0.75):\n",
        "    \"\"\"\n",
        "    Determine the lower and upper outlier thresholds using the Interquartile Range (IQR) method.\n",
        "\n",
        "    Args:\n",
        "    - dataframe: DataFrame containing the data\n",
        "    - col_name: Name of the column for which thresholds are calculated\n",
        "    - th1: Lower quartile (default is 0.25)\n",
        "    - th3: Upper quartile (default is 0.75)\n",
        "\n",
        "    Returns:\n",
        "    - lower_limit: Lower outlier threshold\n",
        "    - upper_limit: Upper outlier threshold\n",
        "    \"\"\"\n",
        "    quartile1 = dataframe[col_name].quantile(th1)\n",
        "    quartile3 = dataframe[col_name].quantile(th3)\n",
        "    iqr = quartile3 - quartile1\n",
        "    upper_limit = quartile3 + 1.5 * iqr\n",
        "    lower_limit = quartile1 - 1.5 * iqr\n",
        "    return lower_limit, upper_limit\n",
        "#Determine the Outliers\n",
        "def check_outliers_iqr(dataframe, col_name):\n",
        "    \"\"\"\n",
        "    Check for outliers using the Interquartile Range (IQR) method.\n",
        "\n",
        "    Args:\n",
        "    - dataframe: DataFrame containing the data\n",
        "    - col_name: Name of the column for which outliers are checked\n",
        "\n",
        "    Returns:\n",
        "    - True if outliers are present, False otherwise\n",
        "    \"\"\"\n",
        "    lower_limit, upper_limit = determine_outlier_thresholds_iqr(dataframe, col_name)\n",
        "    if dataframe[(dataframe[col_name] > upper_limit) | (dataframe[col_name] < lower_limit)].any(axis=None):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "#If outlier then replace to the normalised values.\n",
        "def replace_with_thresholds_iqr(dataframe,cols, th1=0.05, th3=0.95, replace=False):\n",
        "    \"\"\"\n",
        "    Replace outliers with the threshold values using the Interquartile Range (IQR) method.\n",
        "\n",
        "    Args:\n",
        "    - dataframe: DataFrame containing the data\n",
        "    - cols: List of columns for which outliers are checked and replaced\n",
        "    - th1: Lower quartile (default is 0.05)\n",
        "    - th3: Upper quartile (default is 0.95)\n",
        "    - replace: Boolean indicating whether to replace outliers or not (default is False)\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    for col_name in cols:\n",
        "        if col_name != 'Schooling':\n",
        "            outliers_ = check_outliers_iqr(dataframe,col_name)\n",
        "            count = None\n",
        "            lower_limit, upper_limit = determine_outlier_thresholds_iqr(dataframe, col_name, th1, th3)\n",
        "            if outliers_:\n",
        "                count = dataframe[(dataframe[col_name] > upper_limit) | (dataframe[col_name] < lower_limit)][col].count()\n",
        "                if replace:\n",
        "                    if lower_limit < 0:\n",
        "                        # We donnot want to replace with negative values.\n",
        "                        dataframe.loc[(dataframe[col_name] > upper_limit), col_name] = upper_limit\n",
        "                    else:\n",
        "                        dataframe.loc[(dataframe[col_name] < lower_limit), col_name] = lower_limit\n",
        "                        dataframe.loc[(dataframe[col_name] > upper_limit), col_name] = upper_limit\n",
        "            outliers_status = check_outliers_iqr(data, col_name)\n",
        "            data.append([outliers_, outliers_status, count, col_name, lower_limit, upper_limit ])\n",
        "            table = tabulate(data, headers=['Outliers (Previously)', 'Outliers', 'Count', 'Column', 'Lower Limit', 'Upper Limit'], tablefmt='rst', numalign='right')\n",
        "            print(\"Removing Outliers using IQR\")\n",
        "            print(table)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f5378b3",
      "metadata": {
        "id": "0f5378b3"
      },
      "outputs": [],
      "source": [
        "#Using data and columns\n",
        "replace_with_thresholds_iqr(data, data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16038a7c",
      "metadata": {
        "id": "16038a7c"
      },
      "outputs": [],
      "source": [
        "# Check the updated distribution after outlier removal\n",
        "plt.figure(figsize=(30,40))\n",
        "for i, col in enumerate(count_vals, start=1):\n",
        "    plt.subplot(6, 4, i)\n",
        "    data.boxplot(col)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "109b40fa",
      "metadata": {
        "id": "109b40fa"
      },
      "source": [
        "As there is still some Outliers left, we can ignore them as they aren't going to highly effect our Model.\n",
        "We observe that Measles and Population have quite a few outliers. Of these, we suspect that they will not be very useful as estimators: measles is not a lethal disease in the vast majority of cases (nor does it harm victims much compared to other diseases deadliest), and the population we suspect will not affect quality of life much (we will later study how correlated it is with this objective metric)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "435ba78c",
      "metadata": {
        "id": "435ba78c"
      },
      "outputs": [],
      "source": [
        "# Check the updated distribution after outlier removal\n",
        "plt.figure(figsize=(30, 40))\n",
        "for i, col in enumerate(data.columns):\n",
        "    plt.subplot(6, 4, i + 1)\n",
        "    plt.hist(data[col], alpha=0.3, color='b', density=True)\n",
        "    plt.title(col)\n",
        "    plt.xticks(rotation='vertical')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8363dbf5",
      "metadata": {
        "id": "8363dbf5"
      },
      "outputs": [],
      "source": [
        "#Apply logit transformation for the logit histogram\n",
        "data['Polio']=np.log(((data['Polio'])/(100-data['Polio'])))\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.displot(data['Polio'])\n",
        "plt.show()\n",
        "data['Diphtheria']=np.log(((data['Diphtheria'])/(100-data['Diphtheria'])))\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.displot(data['Diphtheria'])\n",
        "plt.show()\n",
        "\n",
        "print(data['Polio'].skew(),data['Diphtheria'].skew())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c1b25bf",
      "metadata": {
        "id": "5c1b25bf"
      },
      "source": [
        "The logit transformation has a better skewness we will keep that feature."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8655987e",
      "metadata": {
        "id": "8655987e"
      },
      "source": [
        "## 3.2 Additional Transform(Intern.)\n",
        "Here I using the Box Cox power and use it to compare to the logit tranform(only for skew > -1) as a advanced transformation. However, this is not be used at the end based on the instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4de35856",
      "metadata": {
        "id": "4de35856"
      },
      "outputs": [],
      "source": [
        "#function to return the Box Cox power\n",
        "import scipy.stats as stat\n",
        "\n",
        "\n",
        "def bcpwr (y):\n",
        "    skew1 = pd.DataFrame.skew (pd.DataFrame (y))\n",
        "    print (\"original skewness:\", skew1)\n",
        "    ymin = np.min (y)\n",
        "    if ymin <= 0:\n",
        "        offset = np.min (y [y>0]) / 2\n",
        "    else:\n",
        "        offset = 0\n",
        "    bc = stat.boxcox (y + offset)\n",
        "    return (bc [1])\n",
        "#function to plot a histogram of the data, calculate the box cox power, and plot another histogram of the power-transformed feature\n",
        "\n",
        "def plotbc (y, plots=True):\n",
        "\n",
        "    if plots:\n",
        "        sns.histplot (y).set (title=\"Original Data\")\n",
        "\n",
        "    pwr = bcpwr (y)\n",
        "    print (\"power=\", pwr)\n",
        "    ymin = np.min (y)\n",
        "    if ymin <= 0:\n",
        "        offset = np.min (y [y>0]) / 2\n",
        "    else:\n",
        "        offset = 0\n",
        "\n",
        "    if pwr < -0.75:\n",
        "        bcy = -1 * (y + offset) ** -1\n",
        "    elif pwr < -0.25:\n",
        "        bcy = -0.5 * (y + offset) ** -0.5\n",
        "    elif pwr < 0.25:\n",
        "        bcy = np.log (y + offset)\n",
        "    elif pwr < 0.75:\n",
        "        bcy = pwr * y ** 0.5\n",
        "    elif pwr < 1.25:\n",
        "        bcy = y\n",
        "    else:\n",
        "        bcy = y ** 2\n",
        "\n",
        "    if plots:\n",
        "        plt.show()\n",
        "        sns.histplot (bcy).set (title=\"Transformed Data\")\n",
        "        plt.show()\n",
        "\n",
        "    return (bcy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a4b7f5e",
      "metadata": {
        "id": "8a4b7f5e"
      },
      "outputs": [],
      "source": [
        "#transform Alcohol\n",
        "data['transformed_alcohol']= plotbc (data['Alcohol'],plots=True)\n",
        "print(data['transformed_alcohol'].skew())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e5dc884",
      "metadata": {
        "id": "1e5dc884"
      },
      "source": [
        "Alcohol now has a skewness of -0.138. This is better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb9dd0e0",
      "metadata": {
        "id": "eb9dd0e0"
      },
      "outputs": [],
      "source": [
        "#transform Alcohol\n",
        "data['transformed_GDP']= plotbc (data.GDP, plots=True)\n",
        "\n",
        "print(data['transformed_GDP'].skew())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d55396fd",
      "metadata": {
        "id": "d55396fd"
      },
      "outputs": [],
      "source": [
        "data['transformed_population']= plotbc (data['Population'], plots=True)\n",
        "print(data['transformed_population'].skew())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e869244",
      "metadata": {
        "id": "0e869244"
      },
      "outputs": [],
      "source": [
        "data['transformed_Thinness10-19years']= plotbc (data['Thinness10-19years'], plots=True)\n",
        "print(data['transformed_Thinness10-19years'].skew())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7ab2767",
      "metadata": {
        "id": "d7ab2767"
      },
      "outputs": [],
      "source": [
        "data['transformed_Thinness5-9years']= plotbc (data['Thinness5-9years'], plots=True)\n",
        "print(data['transformed_Thinness5-9years'].skew())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0e0a32e",
      "metadata": {
        "id": "d0e0a32e"
      },
      "outputs": [],
      "source": [
        "data['transformed_AdultMortality']= plotbc (data['AdultMortality'], plots=True)\n",
        "print(data['transformed_AdultMortality'].skew())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2115b520",
      "metadata": {
        "id": "2115b520"
      },
      "outputs": [],
      "source": [
        "data['transformed_AdultMortality-Male']= plotbc (data['AdultMortality-Male'], plots=True)\n",
        "print(data['transformed_AdultMortality-Male'].skew())\n",
        "data['transformed_AdultMortality-Female']= plotbc (data['AdultMortality-Female'], plots=True)\n",
        "print(data['transformed_AdultMortality-Female'].skew())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa168437",
      "metadata": {
        "id": "aa168437"
      },
      "outputs": [],
      "source": [
        "#logit transformation of Hepatitis B\n",
        "data['logit_polio']=np.log(((data['Polio'])/(100-data['Polio'])))\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.displot(data['logit_polio'])\n",
        "plt.show()\n",
        "print(data['logit_polio'].skew())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ef5081f",
      "metadata": {
        "id": "4ef5081f"
      },
      "source": [
        "Logit Polio has a skewess of -0.981."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dea848f6",
      "metadata": {
        "id": "dea848f6"
      },
      "outputs": [],
      "source": [
        "data['transformed_polio']= plotbc (data['Polio'], plots=True)\n",
        "print(data['transformed_polio'].skew())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b37a5cfe",
      "metadata": {
        "id": "b37a5cfe"
      },
      "source": [
        "The logit transformation has a better skewness we will keep that feature and drop the boxcox transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aae4d27",
      "metadata": {
        "id": "8aae4d27"
      },
      "outputs": [],
      "source": [
        "data['logit_diphtheria']=np.log(((data['Diphtheria'])/(100-data['Diphtheria'])))\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.displot(data['logit_diphtheria'])\n",
        "plt.show()\n",
        "\n",
        "data['transformed_diptheria']= plotbc (data['Diphtheria'], plots=True)\n",
        "print(data['logit_diphtheria'].skew())\n",
        "print(data['transformed_diptheria'].skew())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a7fb255",
      "metadata": {
        "id": "8a7fb255"
      },
      "source": [
        "The logit transformation has a better skewness we will keep that feature and drop the boxcox transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de385e96",
      "metadata": {
        "id": "de385e96"
      },
      "outputs": [],
      "source": [
        "data['logit_income']=np.log(((data['IncomeCompositionOfResources']+1)/(100-(data['IncomeCompositionOfResources']+2))))\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.displot(data['logit_income'])\n",
        "plt.show()\n",
        "\n",
        "data['transformed_income']= plotbc (data['IncomeCompositionOfResources'], plots=True)\n",
        "print(data['logit_income'].skew())\n",
        "print(data['transformed_income'].skew())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cacdc8f1",
      "metadata": {
        "id": "cacdc8f1"
      },
      "source": [
        "The boxcox transformation of income has a better skewness and distribution we will keep this transformation and drop logit of income."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8b92e11",
      "metadata": {
        "id": "c8b92e11"
      },
      "outputs": [],
      "source": [
        "data['logit_Schooling']=np.log(((data['Schooling']+1)/(100-(data['Schooling']+2))))\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.displot(data['logit_Schooling'])\n",
        "plt.show()\n",
        "\n",
        "data['transformed_Schooling']= plotbc (data['Schooling'], plots=True)\n",
        "\n",
        "print(data['logit_Schooling'].skew())\n",
        "print(data['transformed_Schooling'].skew())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "546aecfd",
      "metadata": {
        "id": "546aecfd"
      },
      "source": [
        "The boxcox transformation of schooling has a better skewness and distribution we will keep this transformation and drop logit of schooling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b9d44c",
      "metadata": {
        "id": "a1b9d44c"
      },
      "outputs": [],
      "source": [
        "data['transformed_Under5LS']= plotbc (data['Under5LS'], plots=True)\n",
        "print(data['transformed_Under5LS'].skew())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61a5ad5c",
      "metadata": {
        "id": "61a5ad5c"
      },
      "outputs": [],
      "source": [
        "data['transformed_HIV-AIDS']= plotbc (data['HIV-AIDS'], plots=True)\n",
        "print(data['transformed_HIV-AIDS'].skew())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e1d5cac",
      "metadata": {
        "id": "3e1d5cac"
      },
      "outputs": [],
      "source": [
        "data['transformed_PercentageExpenditure']= plotbc (data['PercentageExpenditure'], plots=True)\n",
        "print(data['transformed_PercentageExpenditure'].skew())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "427f8fd7",
      "metadata": {
        "id": "427f8fd7"
      },
      "outputs": [],
      "source": [
        "data['transformed_Measles']= plotbc (data['Measles'], plots=True)\n",
        "print(data['transformed_Measles'].skew())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f044a79",
      "metadata": {
        "id": "0f044a79"
      },
      "outputs": [],
      "source": [
        "data['transformed_SLS']= plotbc (data['SLS'], plots=True)\n",
        "print(data['transformed_SLS'].skew())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e24a226",
      "metadata": {
        "id": "8e24a226"
      },
      "outputs": [],
      "source": [
        "#drop initial and some transformed values (those used for comparison)\n",
        "#data.drop('Alcohol', axis = 1, inplace=True)\n",
        "#data.drop('GDP', axis = 1, inplace=True)\n",
        "#data.drop('Population', axis = 1, inplace=True)\n",
        "#data.drop('Thinness10-19years', axis = 1, inplace=True)\n",
        "#data.drop('Thinness5-9years', axis = 1, inplace=True)\n",
        "#data.drop('AdultMortality', axis = 1, inplace=True)\n",
        "#data.drop('AdultMortality-Male', axis = 1, inplace=True)\n",
        "#data.drop('AdultMortality-Female', axis = 1, inplace=True)\n",
        "#data.drop('Polio', axis = 1, inplace=True)\n",
        "#data.drop('transformed_polio', axis = 1, inplace=True)\n",
        "#data.drop('Diphtheria', axis = 1, inplace=True)\n",
        "#data.drop('transformed_diptheria', axis = 1, inplace=True)\n",
        "#data.drop('IncomeCompositionOfResources', axis = 1, inplace=True)\n",
        "#data.drop('logit_income', axis = 1, inplace=True)\n",
        "#data.drop('Schooling', axis = 1, inplace=True)\n",
        "#data.drop('transformed_Schooling', axis = 1, inplace=True)\n",
        "#data.drop('PercentageExpenditure', axis = 1, inplace=True)\n",
        "#data.drop('Measles', axis = 1, inplace=True)\n",
        "#data.drop('Under5LS', axis = 1, inplace=True)\n",
        "#data.drop('HIV-AIDS', axis = 1, inplace=True)\n",
        "data.drop('SLS', axis = 1, inplace=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8914382",
      "metadata": {
        "id": "c8914382"
      },
      "source": [
        "# 4. Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddcff698",
      "metadata": {
        "id": "ddcff698"
      },
      "source": [
        "## 4.1 Data Pre-process"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddaaf1d8",
      "metadata": {
        "id": "ddaaf1d8"
      },
      "source": [
        "One-hot Encoding\n",
        "The following categorical features will be one-hot encoded.\n",
        "Status\n",
        "In general, the attribute country should be encoded but the blind test file may not have all same country data in the train dataset. This will leads to failure to predict test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a378953b",
      "metadata": {
        "id": "a378953b"
      },
      "outputs": [],
      "source": [
        "#dataT = pd.get_dummies(data, columns = ['Country', 'Status',])\n",
        "dataT = pd.get_dummies(data, columns = [ 'Status'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eab5feb5",
      "metadata": {
        "id": "eab5feb5"
      },
      "outputs": [],
      "source": [
        "dataT.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0966f6a",
      "metadata": {
        "id": "e0966f6a"
      },
      "outputs": [],
      "source": [
        "dataT.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4677c3a8",
      "metadata": {
        "id": "4677c3a8"
      },
      "source": [
        "columns number is 24 since we drop ID but split status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4a8398c",
      "metadata": {
        "id": "f4a8398c"
      },
      "outputs": [],
      "source": [
        "dataT.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c774afa0",
      "metadata": {
        "id": "c774afa0"
      },
      "outputs": [],
      "source": [
        "dataT.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a91ad170",
      "metadata": {
        "id": "a91ad170"
      },
      "outputs": [],
      "source": [
        "#Separate the target and the attributes\n",
        "dataT_X = dataT.drop(['TARGET_LifeExpectancy',], axis=1).to_numpy()\n",
        "dataT_y = dataT[['TARGET_LifeExpectancy']].to_numpy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9aed479",
      "metadata": {
        "id": "e9aed479"
      },
      "source": [
        "## 4.2 Data Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ac4af17",
      "metadata": {
        "id": "5ac4af17"
      },
      "outputs": [],
      "source": [
        "#First I neet to split data to train and test\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(dataT_X, dataT_y, test_size=0.2, random_state=0)\n",
        "x_valid, x_test, y_valid, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=0)\n",
        "\n",
        "#See shape of train and test data\n",
        "print(f'X_train shape -->{x_train.shape}')\n",
        "print(f'X_valid shape -->{x_valid.shape}')\n",
        "print(f'X_test shape -->{x_test.shape}')\n",
        "print(f'y_train shape -->{y_train.shape}')\n",
        "print(f'y_valid shape -->{y_valid.shape}')\n",
        "print(f'y_test shape -->{y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db93fe5b",
      "metadata": {
        "id": "db93fe5b"
      },
      "source": [
        "### 4.2.1 Feature scales"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f444c98",
      "metadata": {
        "id": "1f444c98"
      },
      "source": [
        "Since we are planning to use models like linear regression which are sensitive to feature scales, standardization might be more appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf65ecca",
      "metadata": {
        "id": "cf65ecca"
      },
      "outputs": [],
      "source": [
        "# Standardize features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_X_scaled = scaler.fit_transform(x_train)\n",
        "test_X_scaled = scaler.transform(x_test)\n",
        "val_X_scaled = scaler.transform(x_valid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe9e9a79",
      "metadata": {
        "scrolled": true,
        "id": "fe9e9a79"
      },
      "outputs": [],
      "source": [
        "# Check the updated distribution after Standardize features\n",
        "plt.figure(figsize=(30, 40))\n",
        "for i in range(train_X_scaled.shape[1]):\n",
        "    plt.subplot(6, 4, i + 1)\n",
        "    plt.hist(train_X_scaled[:, i], alpha=0.3, color='b', density=True)\n",
        "    plt.title(i)\n",
        "    plt.xticks(rotation='vertical')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a2d22a2",
      "metadata": {
        "id": "5a2d22a2"
      },
      "source": [
        "1'Country', 2'Year', 3'AdultMortality',4'AdultMortality-Male', 5'AdultMortality-Female', 6'SLS', 7'Alcohol',8'PercentageExpenditure', 9'Measles', 10'BMI', 11'Under5LS', 12'Polio',\n",
        "       13'TotalExpenditure', 14'Diphtheria', 15'HIV-AIDS', 16'GDP', 17'Population',\n",
        "       18'Thinness10-19years', 19'Thinness5-9years',\n",
        "       20'IncomeCompositionOfResources', 21'Schooling', 22'Status_0', 23'Status_1']\n",
        "       Now, most of them are standardized."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d992306",
      "metadata": {
        "id": "0d992306"
      },
      "source": [
        "## 4.3 Model Development and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64de0b5d",
      "metadata": {
        "id": "64de0b5d"
      },
      "source": [
        "Metrics Discussion:\n",
        "MAE,MSE and RMSE are loss functions, because we want to minimize them. Here I use one of them and looking the R Square.\n",
        "MAE is the easiest to understand, because it's the average error.\n",
        "MSE is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n",
        "RMSE is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n",
        "\n",
        "RÂ² is particularly useful as it represents the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
        "\n",
        "Hence, I will using MSE and R-squared (RÂ²).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "354d9ac9",
      "metadata": {
        "id": "354d9ac9"
      },
      "source": [
        "### 4.3.1 Model 1 - Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb59079b",
      "metadata": {
        "id": "cb59079b"
      },
      "outputs": [],
      "source": [
        "# 1. Model Training\n",
        "from sklearn.linear_model import LinearRegression\n",
        "linear_reg = LinearRegression()\n",
        "linear_reg = linear_reg.fit(train_X_scaled, y_train)\n",
        "# 2. Model Evaluation\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "val_pred = linear_reg.predict(val_X_scaled)\n",
        "val_rmse = np.sqrt (mean_squared_error(y_valid, val_pred))\n",
        "val_r2 = r2_score(y_valid, val_pred)\n",
        "\n",
        "print(\"Linear Regression Validation Performance:\")\n",
        "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
        "print(f\"Validation R^2 Score: {val_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58104b1b",
      "metadata": {
        "id": "58104b1b"
      },
      "source": [
        "In this case, On average, the model's predictions are off by about 4.7451 units from the actual values. And approximately 72.64% of the variance in the dependent variable (target) is predictable from the independent variables used in the model.\n",
        "\n",
        "Let's train the data in Polynomial regression and compare their peformance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "931e15a6",
      "metadata": {
        "id": "931e15a6"
      },
      "source": [
        "### 4.3.2 Model 2 - Polynomial Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66b62fb6",
      "metadata": {
        "id": "66b62fb6"
      },
      "source": [
        "To test Polynomial Regression, I only try the degrees within 2 and 3. Beacuse it waste the running time when I defining a function in[2,3,4]  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63b4a63a",
      "metadata": {
        "id": "63b4a63a"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "# Add PolynomialFeatures\n",
        "poly_features2 = PolynomialFeatures(2)\n",
        "train_X_poly2 = poly_features2.fit_transform(train_X_scaled)\n",
        "val_X_poly2 = poly_features2.transform(val_X_scaled)\n",
        "test_X_poly2= poly_features2.transform(test_X_scaled)\n",
        "# Train Polynomial Regression Model\n",
        "poly_reg = LinearRegression()\n",
        "poly_reg2= poly_reg.fit(train_X_poly2, y_train)\n",
        "# Make predictions on the validation set\n",
        "val_pred_poly2 = poly_reg2.predict(val_X_poly2)\n",
        "val_rmse_poly2 = np.sqrt (mean_squared_error(y_valid, val_pred_poly2))\n",
        "val_r2_poly2 = r2_score(y_valid, val_pred_poly2)\n",
        "print(f\"Polynomial Regression (2):\")\n",
        "print(f\"Validation RMSE: {val_rmse_poly2:.4f}\")\n",
        "print(f\"Validation R^2 Score: {val_r2_poly2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e84c9cb5",
      "metadata": {
        "id": "e84c9cb5"
      },
      "outputs": [],
      "source": [
        "# Train Polynomial Regression Model\n",
        "poly_features3 = PolynomialFeatures(3)\n",
        "train_X_poly3 = poly_features3.fit_transform(train_X_scaled)\n",
        "val_X_poly3 = poly_features3.transform(val_X_scaled)\n",
        "\n",
        "poly_reg = LinearRegression()\n",
        "poly_reg3= poly_reg.fit(train_X_poly3, y_train)\n",
        "\n",
        "val_pred_poly3 = poly_reg3.predict(val_X_poly3)\n",
        "val_rmse_poly3 =np.sqrt ( mean_squared_error(y_valid, val_pred_poly3))\n",
        "val_r2_poly3 = r2_score(y_valid, val_pred_poly3)\n",
        "print(f\"Polynomial Regression (3):\")\n",
        "print(f\"Validation RMSE: {val_rmse_poly3:.4f}\")\n",
        "print(f\"Validation R^2 Score: {val_r2_poly3:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81c732a6",
      "metadata": {
        "id": "81c732a6"
      },
      "source": [
        "To comparing the evaluation we have currently got, based on the rules that A lower RMSE value indicates better model performance,and R^2 = 1 indicates a perfect fit.\n",
        "Polynomial Regression (2) is chosen."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f1e2420",
      "metadata": {
        "id": "5f1e2420"
      },
      "source": [
        "### 4.3.3 Baseline Model Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e803f25",
      "metadata": {
        "id": "4e803f25"
      },
      "outputs": [],
      "source": [
        "#Looking at the coef and intercept\n",
        "print(\"Parameter of the Polynomial model: \", poly_reg2.coef_)\n",
        "print(\"Intercept of the Polynomial model: \", poly_reg2.intercept_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeaaf222",
      "metadata": {
        "id": "eeaaf222"
      },
      "source": [
        "These coefficients and intercept are essential components of the polynomial model and are used to make predictions based on the input features. They determine the shape and behavior of the polynomial curve fitted to the data during the modeling process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "684374d9",
      "metadata": {
        "id": "684374d9"
      },
      "outputs": [],
      "source": [
        "#Ploting the Diagnostic Plots to verify the accuray of baseline model\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(y_valid, val_pred_poly2, s=25, cmap=plt.cm.coolwarm, zorder=10)\n",
        "\n",
        "lims = [\n",
        "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
        "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
        "]\n",
        "\n",
        "ax.plot(lims, lims, 'k--', alpha=0.75, zorder=0)\n",
        "ax.plot(lims, [np.mean(y_train),]*2, 'r--', alpha=0.75, zorder=0)\n",
        "ax.set_aspect('equal')\n",
        "ax.set_xlim(lims)\n",
        "ax.set_ylim(lims)\n",
        "\n",
        "plt.xlabel('Actual life span')\n",
        "plt.ylabel('Predicted life span')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1c93929",
      "metadata": {
        "id": "f1c93929"
      },
      "source": [
        "Generally, our model is good at predicting life expectancy. There are a few outliers though.  Ideally, most points should cluster around the ideal line, indicating accurate predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b106fda",
      "metadata": {
        "id": "7b106fda"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(y_valid, y_valid-val_pred_poly2, s=25, cmap=plt.cm.coolwarm, zorder=10)\n",
        "\n",
        "xlims = ax.get_xlim()\n",
        "ax.plot(xlims, [0.0,]*2, 'k--', alpha=0.75, zorder=0)\n",
        "ax.set_xlim(xlims)\n",
        "\n",
        "plt.xlabel('Actual life span')\n",
        "plt.ylabel('Residual')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a47aa9e",
      "metadata": {
        "id": "3a47aa9e"
      },
      "source": [
        "We see a few outliers in the graph above, but apart from those there is a nice spread of data points on the predicted vs. residual plot. The large outlier beyond at around -20 is concerning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8721a613",
      "metadata": {
        "id": "8721a613"
      },
      "source": [
        "## 4.4 Apply regularisation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8550aee7",
      "metadata": {
        "id": "8550aee7"
      },
      "source": [
        "Since the previous model gave us nice results for our training set but not that perfect, we can assume that the model was overfit to the train set. To try and fix this, we looked at using a Ridge Regression which is similar to a step-wise regression model and noticed at the workshop3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fd29212",
      "metadata": {
        "id": "4fd29212"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define the Ridge regression model\n",
        "ridge_reg0 = Ridge(alpha=0.1)  # adjust the alpha parameter for different regularization strength\n",
        "\n",
        "# Train the Ridge regression model\n",
        "ridge_reg0.fit(train_X_poly2, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "val_pred_ridge0 = ridge_reg0.predict(val_X_poly2)\n",
        "\n",
        "# Evaluate the model\n",
        "val_rmse_ridge = np.sqrt (mean_squared_error(y_valid, val_pred_ridge0, squared=False))\n",
        "val_r2_ridge = r2_score(y_valid, val_pred_ridge0)\n",
        "\n",
        "print(\"Ridge Regression Validation Performance:\")\n",
        "print(f\"Validation RMSE: {val_rmse_ridge:.4f}\")\n",
        "print(f\"Validation R^2 Score: {val_r2_ridge:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f85a184b",
      "metadata": {
        "id": "f85a184b"
      },
      "source": [
        "The Ridge regression model seems to perform well on the validation set, with lower RMSE and a decent R^2 score compared to the previous model. This indicates that regularization has helped in mitigating overfitting, resulting in a model that generalizes better to unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9d3d326",
      "metadata": {
        "id": "c9d3d326"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define the Ridge regression model\n",
        "ridge_reg0 = Ridge(alpha=0.1)\n",
        "\n",
        "# Train the Ridge regression model\n",
        "ridge_reg0.fit(train_X_poly2, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_pred_ridge0 = ridge_reg0.predict(test_X_poly2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_rmse_ridge = np.sqrt (mean_squared_error(y_test, test_pred_ridge0, squared=False))\n",
        "test_r2_ridge = r2_score(y_test, test_pred_ridge0)\n",
        "\n",
        "print(\"Ridge Regression Test Performance:\")\n",
        "print(f\"Test RMSE: {test_rmse_ridge:.4f}\")\n",
        "print(f\"Test R^2 Score: {test_r2_ridge:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51a95cf3",
      "metadata": {
        "id": "51a95cf3"
      },
      "source": [
        "It looks like the Ridge regression model is performing well both on the validation set and the test set:\n",
        "\n",
        "The validation RMSE (Root Mean Squared Error) is 3.4544, which indicates the average deviation of the predicted life expectancy values from the actual values.\n",
        "The validation R^2 score is 0.8550, which indicates the proportion of the variance in the target variable (life expectancy) that is predictable from the features.\n",
        "These results suggest that the Ridge regression model is providing a good fit to the validation data and is able to explain a significant portion of the variance in the target variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70d66ee7",
      "metadata": {
        "id": "70d66ee7"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(y_valid,y_valid - val_pred_ridge0 , s=25, cmap=plt.cm.coolwarm, zorder=10)\n",
        "\n",
        "xlims = ax.get_xlim()\n",
        "ax.plot(xlims, [0.0,]*2, 'k--', alpha=0.75, zorder=0)\n",
        "ax.set_xlim(xlims)\n",
        "\n",
        "plt.xlabel('Actual life span')\n",
        "plt.ylabel('Residual')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a05d95",
      "metadata": {
        "id": "77a05d95"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(y_test,y_test - test_pred_ridge0 , s=25, cmap=plt.cm.coolwarm, zorder=10)\n",
        "\n",
        "xlims = ax.get_xlim()\n",
        "ax.plot(xlims, [0.0,]*2, 'k--', alpha=0.75, zorder=0)\n",
        "ax.set_xlim(xlims)\n",
        "\n",
        "plt.xlabel('Actual life span')\n",
        "plt.ylabel('Residual')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a095347",
      "metadata": {
        "id": "4a095347"
      },
      "source": [
        "Since the previous model gave us perfect results for our training set and very bad results for our test set,Grid Search could be useful to fixing those outliers datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2be7a784",
      "metadata": {
        "id": "2be7a784"
      },
      "source": [
        "## 4.5 Hyper-parameter setting and tuning\n",
        "### 4.5.1Grid Search for Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02f0abbd",
      "metadata": {
        "id": "02f0abbd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {'alpha': range(0,100)}\n",
        "\n",
        "# Initialize Ridge Regression model\n",
        "ridge_reg = Ridge()\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=ridge_reg, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(train_X_poly2, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Reinitialize Ridge Regression with the best parameters\n",
        "best_alpha = best_params['alpha']\n",
        "best_ridge_reg = Ridge(alpha=best_alpha)\n",
        "best_ridge_reg.fit(train_X_poly2, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46e24a14",
      "metadata": {
        "id": "46e24a14"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define the Ridge regression model\n",
        "ridge_reg100 = Ridge(best_alpha)\n",
        "# Train the Ridge regression model\n",
        "ridge_reg100.fit(train_X_poly2, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_pred_ridge100 = ridge_reg100.predict(test_X_poly2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_rmse_ridge100 = np.sqrt (mean_squared_error(y_test, test_pred_ridge100, squared=False))\n",
        "test_r2_ridge100 = r2_score(y_test, test_pred_ridge100)\n",
        "\n",
        "print(\"Ridge Regression99 Test Performance:\")\n",
        "print(f\"Test RMSE: {test_rmse_ridge100:.4f}\")\n",
        "print(f\"Test R^2 Score: {test_r2_ridge100:.4f}\")\n",
        "\n",
        "# Make predictions on the validation set\n",
        "val_pred_ridge100 = ridge_reg100.predict(val_X_poly2)\n",
        "\n",
        "# Evaluate the model\n",
        "val_rmse_ridge100 = np.sqrt (mean_squared_error(y_valid, val_pred_ridge100, squared=False))\n",
        "val_r2_ridge100 = r2_score(y_valid, val_pred_ridge100)\n",
        "\n",
        "print(\"Ridge Regression99 Validation Performance:\")\n",
        "print(f\"Validation RMSE: {val_rmse_ridge100:.4f}\")\n",
        "print(f\"Validation R^2 Score: {val_r2_ridge100:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5cbff0d",
      "metadata": {
        "id": "d5cbff0d"
      },
      "source": [
        "Now, the performance on test is better and very coles to the perfomance on validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cefa8189",
      "metadata": {
        "id": "cefa8189"
      },
      "source": [
        "### 4.5.2 Backwards Feature Selection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4b37747",
      "metadata": {
        "id": "a4b37747"
      },
      "source": [
        "To considering a better performance, I am tring to select the different dataX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aeddd86",
      "metadata": {
        "id": "4aeddd86"
      },
      "outputs": [],
      "source": [
        "coefficients = linear_reg.coef_\n",
        "\n",
        "# Print coefficients along with corresponding feature names\n",
        "feature_names = dataT.drop('TARGET_LifeExpectancy', axis=1).columns\n",
        "coefficients_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients.flatten()})\n",
        "print(coefficients_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24a98be6",
      "metadata": {
        "id": "24a98be6"
      },
      "outputs": [],
      "source": [
        "# Sort coefficients by absolute value to see the most influential features\n",
        "coefficients_df['AbsoluteCoefficient'] = abs(coefficients_df['Coefficient'])\n",
        "sorted_coefficients = coefficients_df.sort_values(by='AbsoluteCoefficient', ascending=False)\n",
        "print(sorted_coefficients)\n",
        "# Select the top N variables based on absolute coefficient values\n",
        "N = 10 #10 here is ensuring the enough coefficients to avoiding a poor prediction.\n",
        "selected_variables = sorted_coefficients['Feature'].head(N).tolist()\n",
        "\n",
        "print(\"Selected variables based on coefficient magnitude:\")\n",
        "print(selected_variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f9677ff",
      "metadata": {
        "id": "9f9677ff"
      },
      "outputs": [],
      "source": [
        "#Again\n",
        "dataT_X2 = dataT[selected_variables].to_numpy()\n",
        "\n",
        "#Split data with the same ratio 8:1:1\n",
        "x2_train, x2_test, y2_train, y2_test = train_test_split(dataT_X2, dataT_y, test_size=0.2, random_state=0)\n",
        "x2_valid, x2_test, y2_valid, y2_test = train_test_split(x2_test, y2_test, test_size=0.5, random_state=0)\n",
        "\n",
        "# see shape of train and test data\n",
        "print(f'X2_train shape -->{x2_train.shape}')\n",
        "print(f'X2_valid shape -->{x2_valid.shape}')\n",
        "print(f'X2_test shape -->{x2_test.shape}')\n",
        "print(f'y2_train shape -->{y2_train.shape}')\n",
        "print(f'y2_valid shape -->{y2_valid.shape}')\n",
        "print(f'y2_test shape -->{y2_test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c8ff89",
      "metadata": {
        "scrolled": true,
        "id": "04c8ff89"
      },
      "outputs": [],
      "source": [
        "scaler2 = StandardScaler()\n",
        "train_X2_scaled = scaler2.fit_transform(x2_train)\n",
        "test_X2_scaled = scaler2.transform(x2_test)\n",
        "val_X2_scaled = scaler2.transform(x2_valid)\n",
        "\n",
        "poly2 = PolynomialFeatures(2)\n",
        "train_X2_poly2 = poly2.fit_transform(train_X2_scaled)\n",
        "test_X2_poly2 = poly2.transform(test_X2_scaled)\n",
        "val_X2_poly2= poly2.transform(val_X2_scaled)\n",
        "\n",
        "# Define the Ridge regression model\n",
        "ridge_regX2 = Ridge(best_alpha)\n",
        "\n",
        "# Train the Ridge regression model\n",
        "ridge_regX2.fit(train_X2_poly2, y2_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_pred_ridgeX2 = ridge_regX2.predict(test_X2_poly2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_rmse_ridgeX2 = mean_squared_error(y2_test, test_pred_ridgeX2, squared=False)\n",
        "test_r2_ridgeX2 = r2_score(y2_test, test_pred_ridgeX2)\n",
        "\n",
        "print(\"Ridge Regression TestX2 Performance:\")\n",
        "print(f\"Test RMSE: {test_rmse_ridgeX2:.4f}\")\n",
        "print(f\"Test R^2 Score: {test_r2_ridgeX2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cac1caee",
      "metadata": {
        "id": "cac1caee"
      },
      "source": [
        " In this case, we will keep the previous ridge100 Model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c27b3e64",
      "metadata": {
        "id": "c27b3e64"
      },
      "source": [
        "## 4.6 5-fold Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21eda33e",
      "metadata": {
        "id": "21eda33e"
      },
      "source": [
        "TO verify the decision I have made, the k-fold Cross Validation is using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d43fd399",
      "metadata": {
        "id": "d43fd399"
      },
      "outputs": [],
      "source": [
        "# 5 fold cross validation\n",
        "from sklearn.model_selection import cross_val_score,cross_validate\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Define the MSE scorer for cross-validation\n",
        "mse_scorer = make_scorer(mean_squared_error)\n",
        "\n",
        "# Define the range of alpha parameters\n",
        "alpha_values = np.logspace(-5, 5, num=10)\n",
        "\n",
        "cv_results = dict()\n",
        "\n",
        "# Iterate over each alpha parameter\n",
        "for alpha_val in alpha_values:\n",
        "    regressor = Ridge(alpha=alpha_val)\n",
        "\n",
        "    # Perform k-fold cross-validation\n",
        "    scores = cross_val_score(regressor, train_X_poly2, y_train,\n",
        "                             scoring=mse_scorer, cv=5)\n",
        "\n",
        "    cv_results[alpha_val] = scores\n",
        "\n",
        "# Print the mean MSE for each alpha parameter\n",
        "for alpha_val, scores in cv_results.items():\n",
        "    print(f\"Alpha Parameter: {alpha_val}\")\n",
        "    print(f\"Mean MSE: {np.mean(scores)}\")\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "# Plotting mean MSE vs. alpha parameter\n",
        "plt.errorbar(alpha_values, [np.mean(cv_results[alpha_val]) for alpha_val in alpha_values],\n",
        "             yerr=[np.std(cv_results[alpha_val]) for alpha_val in alpha_values], fmt='-o')\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel('Alpha Parameter')\n",
        "plt.ylabel('Mean MSE')\n",
        "plt.title('Mean MSE vs. Alpha Parameter')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7643ed6",
      "metadata": {
        "id": "b7643ed6"
      },
      "source": [
        "As the graph shown above, we are going to finding the minimum point that indicates less overfitting. The X-value in the graph is not clear but the range is from 1 to 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "250996c6",
      "metadata": {
        "id": "250996c6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Define the alpha values to search\n",
        "alphas = range(0,101)\n",
        "\n",
        "# Create a Ridge regression model\n",
        "ridge = Ridge()\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(estimator=ridge, param_grid=dict(alpha=alphas), scoring='neg_mean_squared_error', cv=10)\n",
        "grid_search.fit(train_X_poly2, y_train)  # X and y are the feature matrix and target vector\n",
        "\n",
        "# Get the best alpha\n",
        "best_alpha2 = grid_search.best_params_['alpha']\n",
        "print(best_alpha2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "195679b9",
      "metadata": {
        "id": "195679b9"
      },
      "outputs": [],
      "source": [
        "# Define the Ridge regression model\n",
        "ridge_reg75 = Ridge(best_alpha2)\n",
        "\n",
        "# Train the Ridge regression model\n",
        "ridge_reg75.fit(train_X_poly2, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "test_pred_ridge75 = ridge_reg75.predict(test_X_poly2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_rmse_ridge75 = np.sqrt (mean_squared_error(y_test, test_pred_ridge75, squared=False))\n",
        "test_r2_ridge75 = r2_score(y_test, test_pred_ridge75)\n",
        "\n",
        "print(\"Ridge Regression75 Test Performance:\")\n",
        "print(f\"Test RMSE: {test_rmse_ridge75:.4f}\")\n",
        "print(f\"Test R^2 Score: {test_r2_ridge75:.4f}\")\n",
        "\n",
        "\n",
        "# Make predictions on the validation set\n",
        "val_pred_ridge75 = ridge_reg75.predict(val_X_poly2)\n",
        "\n",
        "# Evaluate the model\n",
        "val_rmse_ridge75 = np.sqrt (mean_squared_error(y_valid, val_pred_ridge75, squared=False))\n",
        "val_r2_ridge75 = r2_score(y_valid, val_pred_ridge75)\n",
        "\n",
        "print(\"Ridge Regression75 Validation Performance:\")\n",
        "print(f\"Validation RMSE: {val_rmse_ridge75:.4f}\")\n",
        "print(f\"Validation R^2 Score: {val_r2_ridge75:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46119e02",
      "metadata": {
        "id": "46119e02"
      },
      "source": [
        "The difference between 75 and 99 is less on both sets.Based on the method I kept 99.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a42226ee",
      "metadata": {
        "id": "a42226ee"
      },
      "source": [
        "# 5. Predict Blind Test\n",
        "After training the best Model that we declared before, the blind test data could be predicted."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f353ac1d",
      "metadata": {
        "id": "f353ac1d"
      },
      "source": [
        "## 5.1 Data Preparing\n",
        "In this steps we are looking at the strcuture of the test data and repeat what we did to matching the sturcture that the model pefered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4b04a36",
      "metadata": {
        "id": "b4b04a36"
      },
      "outputs": [],
      "source": [
        "#Load data\n",
        "filename2 = \"test.csv\"\n",
        "test = pd.read_csv(filename2,delimiter=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b46753",
      "metadata": {
        "scrolled": true,
        "id": "81b46753"
      },
      "outputs": [],
      "source": [
        "test.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a48a2462",
      "metadata": {
        "id": "a48a2462"
      },
      "outputs": [],
      "source": [
        "#status checking\n",
        "print(set((test['Status']).astype(int)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8b99b81",
      "metadata": {
        "id": "b8b99b81"
      },
      "outputs": [],
      "source": [
        "#Missing value\n",
        "test.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5050fc9",
      "metadata": {
        "id": "c5050fc9"
      },
      "outputs": [],
      "source": [
        "test['Population'] = test['Population']/1000000\n",
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "352a4735",
      "metadata": {
        "id": "352a4735"
      },
      "outputs": [],
      "source": [
        "#Rename the columns thinness  1-19 years to match the feature description\n",
        "test.rename(columns={'Thinness1-19years':'Thinness10-19years'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a66e23ae",
      "metadata": {
        "scrolled": true,
        "id": "a66e23ae"
      },
      "outputs": [],
      "source": [
        "testN = test.drop(['ID'], axis=1)\n",
        "testN.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "541f1fb8",
      "metadata": {
        "id": "541f1fb8"
      },
      "outputs": [],
      "source": [
        "# Check the updated distribution after outlier removal\n",
        "plt.figure(figsize=(30,40))\n",
        "for i, col in enumerate(testN.columns):\n",
        "    plt.subplot(6,4,i+1)\n",
        "    plt.hist(data[col], alpha=0.3, color='b', density=True)\n",
        "    plt.title(col)\n",
        "    plt.xticks(rotation='vertical')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3311378c",
      "metadata": {
        "id": "3311378c"
      },
      "outputs": [],
      "source": [
        "#The metod is declared previously\n",
        "replace_with_thresholds_iqr(testN, testN.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7d767b2",
      "metadata": {
        "id": "b7d767b2"
      },
      "outputs": [],
      "source": [
        "#Again using the logit transform\n",
        "testN['Polio']=np.log(((testN['Polio'])/(100-testN['Polio'])))\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.displot(test['Polio'])\n",
        "plt.show()\n",
        "testN['Diphtheria']=np.log(((testN['Diphtheria'])/(100-testN['Diphtheria'])))\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.displot(test['Diphtheria'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3fb9e62",
      "metadata": {
        "id": "e3fb9e62"
      },
      "source": [
        "## 5.2 Apply Training Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31eae8dd",
      "metadata": {
        "id": "31eae8dd"
      },
      "source": [
        "We will using the ridge Model within alpha 99."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b085946e",
      "metadata": {
        "id": "b085946e"
      },
      "outputs": [],
      "source": [
        "#We only choosing the attribute Status here.\n",
        "testT= pd.get_dummies(testN, columns = [ 'Status'])\n",
        "testT.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62d1568a",
      "metadata": {
        "id": "62d1568a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Standardize the features using the same scaler used during training\n",
        "blind_test_scaled = scaler.transform(testT)\n",
        "\n",
        "# Apply polynomial transformation\n",
        "blind_test_poly2 = poly_features2.transform(blind_test_scaled)\n",
        "\n",
        "# Predict life expectancy for the standardized and transformed blind test data\n",
        "predictions = ridge_reg100.predict(blind_test_poly2)\n",
        "\n",
        "# Reshape predictions array to 1-dimensional\n",
        "predictions = predictions.ravel()\n",
        "\n",
        "# Create a DataFrame with ID and predicted life expectancy\n",
        "predictions_df = pd.DataFrame({'ID': test['ID'], 'LifeExpectancy': predictions})\n",
        "\n",
        "# Save or display the predictions\n",
        "print(predictions_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "380e824f",
      "metadata": {
        "id": "380e824f"
      },
      "source": [
        "## 5.3 Prediction vertification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d6617cc",
      "metadata": {
        "id": "9d6617cc"
      },
      "outputs": [],
      "source": [
        "#Distance plot for Life Expectancy column\n",
        "sns.distplot(predictions_df['LifeExpectancy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a97b37c5",
      "metadata": {
        "id": "a97b37c5"
      },
      "outputs": [],
      "source": [
        "#find the skewness of life expectancy\n",
        "print(predictions_df['LifeExpectancy'].skew())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2411868",
      "metadata": {
        "id": "e2411868"
      },
      "source": [
        "To compare to the target LifeExpectancy, the predisction almost fit in the same distribution. However the range of prediction may less about 5 units than original one. Let's look at the ridge75 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5abe381e",
      "metadata": {
        "id": "5abe381e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Predict life expectancy for the standardized and transformed blind test data\n",
        "predictions75 = ridge_reg75.predict(blind_test_poly2)\n",
        "\n",
        "# Reshape predictions array to 1-dimensional\n",
        "predictions75 = predictions75.ravel()\n",
        "\n",
        "# Create a DataFrame with ID and predicted life expectancy\n",
        "predictions75_df = pd.DataFrame({'ID': test['ID'], 'LifeExpectancy': predictions75})\n",
        "\n",
        "# Save or display the predictions\n",
        "sns.distplot(predictions75_df['LifeExpectancy'])\n",
        "print(predictions75_df['LifeExpectancy'].skew())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "142c4a02",
      "metadata": {
        "id": "142c4a02"
      },
      "source": [
        "In this case, although the range not really changed but the skew of 75 is less than 100 and more cloes to the train data distribution. In this case, I kept the final 75 prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70f2f353",
      "metadata": {
        "id": "70f2f353"
      },
      "source": [
        "## 5.4 Output to a CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46c936dd",
      "metadata": {
        "id": "46c936dd"
      },
      "outputs": [],
      "source": [
        "predictions75_df.to_csv('predictions.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "782.133px",
        "left": "32px",
        "top": "156.333px",
        "width": "395.067px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}